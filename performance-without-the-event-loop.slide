High performance servers without the event loop
OSCON, Portland
21 Jul 2015

Dave Cheney
dave@cheney.net
http://dave.cheney.net/
@davecheney

* whoami(1)

- Sysadmin before Devops
- Transitioned to Go 5 years ago
- Work for Canonical contributing to the Go project

* Who is this presentation for ?

.image performance-without-the-event-loop/pager.jpg _ 400
.caption The horror!

As an admin in a past life, the most stressful times in my career were defined by unsatisfying performance.

I got into Go because of its potential to build high performance servers.

As we're in a technical track, I'm not here to _tell_ you that Go is fast, instead I'm going to _explain_ why Go is fast.

* An argument for an efficient programming language

# But before I get down to the technical parts, I want to lay out two justifications for the market that Go is targeting

* Moore's Law

# The oft mis quoted

.image performance-without-the-event-loop/CPU.png _ 320
.caption Image credit: [[http://www.gotw.ca/publications/concurrency-ddj.htm][Herb Sutter (Dr. Dobb's Journal, March 2005)]]

Moore's law states that the number of transistors per square inch doubles roughly every 18 months.

However, clock speeds topped out a decade ago with the Pentium 4 (Penryn) and have been slipping backwards since.

# However clock speeds, which are a function of entirely different properties topped out a decade ago with the Penryn generation ~ 3.5Ghz and have been slipping backwards ever since

* From space constrained to power constrained

.image performance-without-the-event-loop/sun-ultra-enterprise-450-400mhz-2gb-20-bay-workgroup-server-system-no-hdd-parts_131514071457.jpg
.caption Image credit: [[http://www.ccmostwanted.com/store/Sun-Ultra-Enterprise-450-400mhz-2gb-20-bay-Workgroup-Server-System-No-Hdd-Parts_131514071457.html][eBay]]

Sun Enterprise e450—about the size of a bar fridge, about the same power consumption.

# This is the Sun e450. When I started my career, these were the workhorses of the industry. This is Internet 1.0 and everyone bought Sun and Cisco. These things were massive, three of them, stacked one on top of another would consume an entire 19" Rack. They only consumed about 500 Watts each.

# Over the last decade, data centers have moved from space constrained, to power constrained. The last two data center rollouts I did, we ran out of power when the rack was barely 1/3rd full.

# Being power constrained has effects at the macro level; can't get enough power for a rack of 1200 Watt 1RU servers. And at the micro level -- all this power, hundreds of watts is being dissipated in microscopic CPU die.

# Space is no longer a problem, because compute densities have improved so rapidly. However, modern CPUs consume more power, significantly more power, in a much smaller area, making cooling more critical, and harder.

* Where does this power consumption come from ?

.image performance-without-the-event-loop/CMOS_Inverter.svg _ 200
.caption Source: [[https://en.wikipedia.org/wiki/CMOS][Wikipedia]]

# Simplest possible gate, a NOT gate, an inverter. If we have a high on the input, the output is low, if we have a low on the input, the output is high.

CMOS stands for Complementary Metal Oxide Semiconductor, the _complementary_ part is the key.

# Previous PMOS or NMOS chips consumed power when the gate was on or off (depending on the technology), CMOS improved on this.

# Digital logic inside a CPU is implemented with pairs of transistors.

When the circuit is on or off, no current flows directly from the source to the drain. However, during transitions there is a brief period where _both_ transistors are conducting.

# Additional power consumption comes from charging the output capacitence

Power consumption, and thus heat dissipation, is directly proportional to number of transition per second—Clock speed.

# https://www.ece.cmu.edu/~ece322/LECTURES/Lecture13/Lecture13.03.pdf

# Leakage current, which is static power dissipation increases as the size of the transistor decreases

* Right, thanks for the non sequitur

CPU feature size reductions are primarily aimed at reducing power consumption.

Reducing power consumption doesn't just mean "green". The primary goal is to keep power consumption, and thus heat dissipation, below levels that will damage the CPU.

Conversely, performance improvements come mainly from microarchitecture tweaks and esoteric vector instructions, which are not directly useful for the general computation.

Added up, each _microarchitecture_ (5 year cycle) change yields at most 10% improvement per generation, and most recently 4-6%.

# With clock speeds fixed, and in direct conflict to total thermal wattage (TDP), achtectural improvements to the core of the cpu, have given only 10% improvements by microachictecture (one every 5 years), and recently as little as 4-6%

Moore's Law is still in effect, but for the people in this room, the free lunch is over.

# Moore's law is still very much in effect, the number of transitors on a CPU continues to increase, but for the people in this room, that's cold comfort because even with smaller feature sizes and more transitors, clock speeds have stabalised.

* What's your point ?

So, why am I rambling on about hardware at a software conference ?

# It is clear to me, and hopefully clear to you now, that hardware is not getting any faster.

The old adage that a slow language doesn't matter because hardware is getting faster, does not apply anymore.

If performance and scale is important to you, and arguably it is, as you're here in this session, then you'll agree with me that the days of throwing hardware at the problem are over - at least in the conventional sense.

You need a language which is efficient, because inefficient languages just do not justify themselves in production, at scale, on a capex basis.

* An argument for a concurrent programming language

* CPUs are not getting faster, but they are getting wider

.image performance-without-the-event-loop/Ivy-Bridge_Die_Flat-HR.jpg _ 700
.caption Image Credit: Intel / [[http://www.anandtech.com/show/5875/dual-coregt2-ivy-bridge-die-measured-121mm2][AnandTech]]

So, CPUs are not getting faster, but they are getting wider.

- Hyper threading
- More cores, dual core on mobile parts, quad core on desktop parts, even more cores on server parts.

# So this is where the transistors are going on the hardware side, and it shouldn't be a great surprise to many of you in the room. Herb Sutter was talking about this a decade ago, Dual CPU has been a thing since the late 1990's with the Pentium Pro. Intel uses Hyper threading to artificially segment the market for processors, Oracle and Fujitsu apply hyper threading more aggressively to their product line, 8 or 16 hardware threads per core.

# To make my argument, I want to walk through some of the highlights of scaling server software.

* Go on the server

A common refrain when talking about Go is it's a language that works well on the server; static binaries, powerful concurrency, and high performance.

This talk focuses on the last two items, how the language and the runtime transparently let Go programmers write highly scalable network servers, without having to worry about thread management or blocking I/O.

# I'm also going to focus on the Intel world, because that is the platform that many of you will be deploying in production.

# But--If you want to talk about other architectures, especially ppc64 and arm64, I'm here all week, and it won't take much to get me talking.

* Processes, threads and Goroutines

# Go has goroutines. These are the foundations for concurrency in Go.

# I want to step back for a moment and explore the history that leads us to goroutines.

* Your grandfather's server

.image performance-without-the-event-loop/3357832896_896d98bbaf_z.jpg
.caption Image Credit: [[https://www.flickr.com/photos/tascott/3357832896/in/album-72157615257588587/][Tom Scott]] (CC BY-NC-SA 2.0)

The first web server, [[http://info.cern.ch/Proposal.html][circa March 1989]].

NCSA was the web server, which grew into Apache.

# This represented the state of the art, at the turn of the century, web serving technology.

# dialup modems

# disk io was the main factor, network io was slower than disk seek times, so raid volumes worked well, and machines like the e450 were workhorses

# http serving was apache, apache, apache, and apache 1.3 btw.

# prefork was the name of the game

* Processes

In the beginning computers ran one process at a time. Then in the 60’s the idea of multiprocessing, or time sharing became popular.

By the 70's this idea was well established for network servers, ftp(1), telnet(1), rlogin(1).

This was the world of Berners-Lee's NCSA Mosaic server running on a 25Mhz Next Cube, every active HTTP session was handled by its own process.

In a time-sharing system the operating systems maintains the illusion of concurrency by rapidly switching the attention of the CPU between active processes by recording the state of the current process, then restoring the state of another.

This is called context switching.

* Process context switching

.image performance-without-the-event-loop/640px-Table_of_x86_Registers_svg.svg.png
.caption Image credit: [[https://commons.wikimedia.org/wiki/File:Table_of_x86_Registers_svg.svg#/media/File:Table_of_x86_Registers_svg.svg][Immae (CC BY-SA 3.0)]]
There are three main costs of a context switch.

- The kernel needs to store the contents of all the CPU registers for that process, then restore the values for another process.
- The kernel needs to flush the CPU’s virtual memory to physical mappings (TLB).
- Overhead of the operating system context switch, and the overhead of the scheduler function to choose the next process to occupy the CPU.

# Because a process switch can occur at any point in a process’ execution, the operating system needs to store the contents of all of these registers because it does not know which are currently in use.

# There are a surprising number of registers in a modern processor. I have difficulty fitting them on one slide, which should give you a clue how much time it takes to save and restore them.

# These costs are relatively fixed by the hardware, and depend on the amount of work done between context switches to ameliorate their cost -- rapid context switching tends to overwhelm the amount of work done between context switches.

* Threads

This lead to the development of threads, which are conceptually the same as processes, but share the same memory space.

As threads share address space, they are lighter than processes, so they are faster to create and faster to switch between.

Threads still have an expensive context switch cost, a lot of state must be retained.

Goroutines take the idea of threads a step further.

* Goroutines

Goroutines are cooperatively scheduled, rather than relying on the kernel to manage their time sharing.

The compiler knows the registers which are in use and saves them automatically.

The switch between goroutines only happens at well defined points, when an explicit call is made to the Go runtime scheduler.

- Channel send and receive operations, if those operations would block.
- The Go statement, although there is no guarantee that new goroutine will be scheduled immediately.
- Blocking syscalls like file and network operations.
- After being stopped for a garbage collection cycle.

In other words, places where the goroutine cannot continue until it has more data, or more space to put data.

* Goroutines

Many goroutines are multiplexed onto a single operating system thread.

- Super cheap to create.
- Super cheap to switch between as it all happens in user space.
- Tens of thousands of goroutines in a single process are the norm, hundreds of thousands not unexpected.

This results in relatively few operating system threads per Go process, with the Go runtime taking care of assigning a runnable Goroutine to a free operating system thread.

* Goroutine example

.code -numbers performance-without-the-event-loop/grep.go /^func grep/,/^}/

* Go uses a M:N scheduler in user space.

If you lived through green threads in Java or user space threads on Linux, then you may be feeling uncomfortable at this point. Let me assure you that in practice this user space scheduler works well. This is because it is integrated with the runtime.

A small number of operating system threads service runnable goroutines

- Go versions 1.4 and lower, defaults to 1
- Go versions 1.5 and above, defaults to the number of CPUs visible to the operating system.

Compare this to threaded applications, where a thread can be preempted at any time, at any instruction. In Go, the compiler handles this as a natural byproduct of the function call preamble.

From the point of view of the language, scheduling looks like a function call, and has the same function call semantics. The thread of execution calls into the scheduler with a specific goroutine stack, but may return with a different goroutine stack.

* Stack management

# In the previous section I discussed how goroutines reduce the overhead of managing many, sometimes hundreds of thousands of concurrent threads of execution.
# There is another side to the goroutine story, and that is stack management.

* Process address space

.image performance-without-the-event-loop/process.png

This is a diagram of the memory layout of a process. The key thing we are interested is the location of the heap and the stack.

# Traditionally inside the address space of a process, the heap is at the bottom of memory, just above the program (text) and grows upwards.

# The stack is located at the top of the virtual address space, and grows downwards.

* Stacks and guard pages

.image performance-without-the-event-loop/guard-page.png

Because the heap and stack overwriting each other would be catastrophic, the operating system arranges an area of unwritable memory between the stack and the heap.

# This is called a guard page, and effectively limits the stack size of a process, usually in the order of several megabytes.

* Thread stacks

.image performance-without-the-event-loop/threads.png

Threads share the same address space, so for each thread, it must have its own stack, and its own guard page.

# Because it is hard to predict the stack requirements of a particular thread, a large amount of memory is reserved for each thread’s stack along with a guard page.

# The hope is that this is more than will ever be needed and the guard page will never be hit.

# The downside is that as the number of threads in your program increases, the amount of available address space is reduced.

* Goroutine stack management

The early process model, allowed the programmer to view the heap and the stack as effectively infinite. The downside was a complicated and expensive subprocess model.

Threads improved the situation a bit, but require the programmer to _guess_ the most appropriate stack size; too small, your program will abort, too large, you run out of virtual address space.

We’ve seen that the Go runtime schedules a large number of goroutines onto a small number of threads, but what about the stack requirements of those goroutines ?

* Goroutine stack growth

.image performance-without-the-event-loop/stack-growth.png 250 _

Each goroutine starts with a small stack, allocated from the heap. The size has fluctuated over time, but in Go 1.5 each goroutine starts with a 2k allocation.

Instead of using guard pages, the Go compiler inserts a check as part of every function call to test if there is sufficient stack for the function to run.

# If there is sufficient stack space, the function runs as normal.

If there is insufficient space, the runtime will allocate a large stack segment on the heap, copy the contents of the current stack to the new segment, free the old segment, and the function call restarted.

Because of this check, a goroutines initial stack can be made much smaller, which in turn permits Go programmers to treat goroutines as cheap resources.

* Integrated network poller

* Dan Kegel's c10k problem

[[http://www.kegel.com/c10k.html]] circa 2002

Question: How to hold up 10,000 concurrent TCP sessions on commodity hardware of the day.

Conventional wisdom suggested that high performance servers require native threads, or more recently, event loops.

Threads carry a high overhead in terms of scheduling cost and memory footprint.

Event loops ameliorate those costs, but introduce their own requirements for a complex callback driven style.

Go provides the best of both worlds.

* Go's answer to c10k

In Go, syscalls are _usually_ blocking operations; `read(2)`, `write(2)`.

Scheduler would let the goroutines' backing thread block, then find or spawn another to continue to service other goroutines.

This works well for file IO as a small number of blocking threads can exhaust your local IO bandwidth.

For network sockets, almost all of your goroutines are going to be blocked waiting for network IO.

In a naive implementation this would mean as many threads as goroutines blocked waiting on network IO.

* Socket Read / Write

.code performance-without-the-event-loop/read.go

* Integrated poller

In older versions of Go, this was handled by a single goroutine which used select(2) / kqueue(2) / epoll(2), IOCP (windows).

In current versions of Go this has been integrated into the runtime.

The runtime knows which goroutine is waiting for the socket to become ready and can put it back on the same CPU.

Async file IO is a slightly harder problem due to spotty operating system support. Maybe for Go 1.6.

* Conclusion

* Goroutines, stack management, and an integrated network poller

Goroutines provide a powerful abstraction that frees the programmer from worrying about thread pools or event loops.

The stack of a goroutine is as big as it needs to be without being concerned about sizing thread stacks or thread pools.

The integrated network poller lets Go programmers avoid convoluted callback styles while still leveraging the most efficient IO completion logic available with the operating system.

The runtime makes sure that there will be just enough threads to service all your goroutines and keep your cores active.

All of these features are transparent to the programmer.

* But wait, there's more

As this is the performance track, I've mostly constrained my remarks to this area, but there is a lot more to the Go success story than just being fast, and these are arguably the more notable features of the language.

- Super fast compilation.
- Excellent tooling; go build, go test, go fmt, go lint, go vet, godoc, -race detector.
- Excellent cross platform support, cross compilation without tears
- Single static binary enables a ridiculously simple deployment story, `scp(1)` and you're done -- no runtime interpreter required.

* Success stories

- Vitess (mysql proxy that sits between every db call behind Youtube)
- dl.google.com
- New York Times
- Hailo
- Facebook/Parse
- Docker
- CoreOS; fleet, etcd, rkt
- Kubernetes
- Baidu
- Qihoo 360
- [[https://github.com/golang/go/wiki/GoUsers][... and many more]]

* Would you like to know more ?

Go take the tour

[[https://tour.golang.org]]

Go join a meetup

[[https://go-meetups.appspot.com]]

Go watch some more talks

[[http://gophervids.appspot.com]]
