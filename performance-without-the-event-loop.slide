High performance servers without the event loop
OSCON, Portland
21 Jul 2015

Dave Cheney
dave@cheney.net
http://dave.cheney.net/
@davecheney

* Introduction

* whoami(1)

.image performance-without-the-event-loop/moi.jpg _ 300

- Sysadmin before Devops
- Transitioned to Go programmer 5 years ago
- Work for Canonical contributing to the Go project

* Who is this presentation for ?

As an admin in a past life the most stressful times in my career were defined by unsatisfying performance.

I got into Go because it offered performance.

While there were other aspects of the language that appealed to me; minimalism, simplicity, orthogonality; it was the performance that was the deciding factor.

Go is a general purpose language currently in use in a wide range of application domains. But if we look back to 2009, to the introductory material specificaly mentions:

"Go is a programming language designed by Google to help solve Google's problems, and Google has big problems." -- [[https://talks.golang.org/2012/splash.article][Go at Google: Language Design in the Service of Software Engineering]]

# Why did I stop, harder to show performance improvements in absolute numbers,answers became more nuanced.
# Thus isn't today's talk, but the narrative remains the same, go is fast, and efficient, and I'm going to show you how and why this is, and especially how you can reap the benefits of this simply
# As we're in a technical track, I'm not here to tell you that Go is fast, instead I hope to _show_ you that Go is fast.

* Go in the server

I'm also going to focus on the Intel world, because that is the platform that many of you will be deploying in production. 

But--If you want to talk about other architectures, especially ppc64 and arm64, I'm here all week, and it won't take much to get me talking.

Conventional wisdom suggests that high performance servers require native threads, or more recently, event loops.

Threads carry a high overhead in terms of scheduling cost and memory footprint.

Event loops ameliorate those costs, but introduce their own requirements for a complex callback driven style.

# A common refrain when talking about Go is its a language that works well on the server; static binaries, powerful concurrency, and high performance.

* An argument for an efficient programming language

# But before I get down to the technical parts, I want to lay out two justifications for the market that Go is targeting

* Moore's Law

# The oft mis quoted

.image performance-without-the-event-loop/CPU.png _ 320
.caption Image credit: [[http://www.gotw.ca/publications/concurrency-ddj.htm][Herb Sutter (Dr. Dobb's Journal, March 2005)]]

Moore's law states that the number of transistors per square inch doubles roughly every 18 months.

However, clock speeds topped out a decade ago with the Pentium 4 (Penryn) and have been slipping backwards since.

# However clock speeds, which are a function of entirely differnt properties topped out a decade ago with the Penryn generation ~ 3.5Ghz and have been slipping backwards ever since

* From space constrained to power constrained

.image performance-without-the-event-loop/sun-ultra-enterprise-450-400mhz-2gb-20-bay-workgroup-server-system-no-hdd-parts_131514071457.jpg
.caption Image credit: [[http://www.ccmostwanted.com/store/Sun-Ultra-Enterprise-450-400mhz-2gb-20-bay-Workgroup-Server-System-No-Hdd-Parts_131514071457.html][eBay]]

Sun Enterprise e450—about the size of a bar fridge, about the same power consumption.

# This is the Sun e450. When I started my career, these were the workhorses of the industry. This is Internet 1.0 and everyone bought Sun and Cisco. These things were massive, three of them, stacked one on top of another would consume an entire 19" Rack. They only consumed about 500 Watts each.

# Over the last decade, data centers have moved from space constrained, to power constrained. The last two data center rollouts I did, we ran out of power when the rack was barely 1/3rd full.

# Being power constrained has effects at the macro level; can't get enough power for a rack of 1200 Watt 1RU servers. And at the micro level -- all this power, hundreds of watts is being dissipated in microscopic cpu die.

# Space is no longer a problem, because compute densities have improved so rapidly. However, modern CPUs consume more power, signficantly more power, in a much smaller area, making cooling more critical, and harder. 	

* Where does this power consumption come from ?

.image performance-without-the-event-loop/CMOS_Inverter.svg _ 200
.caption Source: [[https://en.wikipedia.org/wiki/CMOS][Wikipedia]]

# Simplest possible gate, a NOT gate, an inverter. If we have a high on the input, the output is low, if we have a low on the input, the output is high.

CMOS stands for _complementary_ metal oxide semiconductor, the _complementary_ part is the key.

# Previous PMOS or NMOS chips consumed power when the gate was on or off (depending on the technology), CMOS improved on this.

# Digital logic inside a CPU is implemented with pairs of transistors. 

When the circuit is on or off, no current flows directly from the source to the drain. However during transition there is a brief period where _both_ transitors are conducting.

# Additional power consumption comes from charging the output capacitence

Power consumption, and thus heat disipation, directly proportional to number of transition per second—Clock speed.

# https://www.ece.cmu.edu/~ece322/LECTURES/Lecture13/Lecture13.03.pdf

# Leakage current, which is static power disipation increases as the size of the transitor decreases

* Right, thanks for the non sequitor

CPU feature size shrink are now primarily aimed at reducing power consumption.

# this gives longer battery life in laptops, lower power consumption in desktop and server parts. And when I say lower, we're not talking about "green" here, we're talking about keeping power consumption, and thus heat disipation below levels that will damage the cpu.

Performance improvements come mainly from microarchitecture tweaks and escoteric vector instructions which are not directly useful for the general computation.

Added up, each _microarchitecture_ (5 year cycle) change yeilds at most 10% improvement per generation, and most recently 4-6%.

# With clock speeds fixed, and in direct conflict to total thermal wattage (TDP), achtectural improvements to the core of the cpu, have given only 10% improvements by microachictecture (one every 5 years), and recenly as little as 4-6%

Moore's Law is still in effect, but for the people in this room, the free lunch is over.

# Moore's law is still very much in effect, the number of transitors on a CPU continues to increase, but for the people in this room, that's cold comfort because even with smaller feature sizes and more transitors, clock speeds have stabalised.

* What's your point ?

So, why am I rambling on about hardware at a software conference ? 

# It is clear to me, and hopefuly clear to you now, that hardware is not getting any faster.

The old addage that a slow language doesn't matter because hardware is getting faster, do not apply anymore.

If performance and scale is important to you, and arguably it is, as you're here in this session, then you'll agree with me that the days of throwing hardware at the problem are over - at least in the conventional sense.

You need a language which is efficient, because interpreted languages just do not justify themselves in production, at scale, on a capex basis.

* An argument for a concurrent programming language 

* CPUs are not getting faster, but they are getting wider

.image performance-without-the-event-loop/Ivy-Bridge_Die_Flat-HR.jpg _ 700
.caption Image Credit: Intel / [[http://www.anandtech.com/show/5875/dual-coregt2-ivy-bridge-die-measured-121mm2][AnandTech]]

So, CPUs are not getting faster, but they are getting wider.

- Hyper threading
- More cores, dual core on mobile parts, quad core on desktop parts, even more cores on server parts.

# So this is where the transistors are going on the hardware side, and it's shouldn't be a great surprise to many of you in the room. Herb Sutter was talking about this a decade ago, Dual CPU has been a thing since the late 1990's with the Pentium Pro. Intel uses Hyper threading to artificually segment the market for processors, Oracle and Fujitsu apply hyperthreading more agressively to their product line, 8 or 16 hardware threads per core.

# To make my argument, I want to walk through some of the highlights of scaling server software.

* Processes, threads and Goroutines

# Go has goroutines. These are the foundations for concurrency in Go.

# I want to step back for a moment and explore the history that leads us to goroutines.

* Your grandfather's server

.image performance-without-the-event-loop/3357832896_896d98bbaf_z.jpg
.caption Image Credit: [[https://www.flickr.com/photos/tascott/3357832896/in/album-72157615257588587/][Tom Scott]] (CC BY-NC-SA 2.0)

The first web server, [[http://info.cern.ch/Proposal.html][circa March 1989]].

NCSA was the web server, which grew into apache.

# This represented the state of the art, at the turn of the century, web serving technology.

# dialup modems

# disk io was the main factor, network io was slower than disk seek times, so raid volumes worked well, and machines like the e450 were workhorses

# http serving was apache, apache, apache, and apache 1.3 btw.

# prefork was the name of the game

* Processes

In the beginning computers ran one process at a time. Then in the 60’s the idea of multiprocessing, or time sharing became popular.

By the 70's this idea was well established for network servers, ftp(1), telnet(1), rlogin(1).

This was the world of Burners-Lee's NCSA Mosaic server running on a 25Mhz Next Cube, every active HTTP session was handled by its own process.

In a time-sharing system the operating systems maintains the illusion of concurrency by raplidly switching the attention of the CPU between active processes by recording the state of the current process, then restoring the state of another.

This is called context switching.

* Process context switching

.image performance-without-the-event-loop/640px-Table_of_x86_Registers_svg.svg.png
.caption Image credit: [[https://commons.wikimedia.org/wiki/File:Table_of_x86_Registers_svg.svg#/media/File:Table_of_x86_Registers_svg.svg][Immae (CC BY-SA 3.0)]] 
There are three main costs of a process switch.

- The kernel needs to store the contents of all the CPU registers for that process, then restore the values for another process.
- The kernel needs to flush the CPU’s virtual memory to physical mappings (TLB).
- Overhead of the operating system context switch, and the overhead of the scheduler function to choose the next process to occupy the CPU.

# Because a process switch can occur at any point in a process’ execution, the operating system needs to store the contents of all of these registers because it does not know which are currently in use.

# There are a surprising number of registers in a modern processor. I have difficulty fitting them on one slide, which should give you a clue how much time it takes to save and restore them.

# These costs are reletively fixed by the hardware, and depend on the amount of work done between context switches to ameliorate their cost -- rapid context switching tends to overwhelm the amount of work done between context switches.

* Threads

This lead to the development of threads, which are conceptually the same as processes, but share the same memory space.

As threads share address space, they are lighter than processes so are faster to create and faster to switch between.

Threads still have an expensive context switch cost, a lot of state must be retained.

Goroutines take the idea of threads a step further.

* Goroutines

Goroutines are cooperatively scheduled, rather than relying on the kernel to manage their time sharing.

The compiler knows the registers which are in use and saves them automatically.

The switch between goroutines only happens at well defined points, when an explicit call is made to the Go runtime scheduler.

- Channel send and receive operations, if those operations would block.
- The Go statement, although there is no guarantee that new goroutine will be scheduled immediately.
- Blocking syscalls like file and network operations.
- After being stopped for a garbage collection cycle.

In other words, places where the program cannot continue until it has more data, or more space to put data.

* Goroutine example

.code performance-without-the-event-loop/grep.go /^func grep/,/^}/

* Go uses a M:N scheduler in user space.

If you lived through green threads in Java or user space threads on Linux, then you may be feeling uncomfortable at this point. Let me assure you that in practice this user space scheduler works well. This is because it is integrated with the langauge.

A small number of operating system threads, in the latest release of Go, 1.5 due at the end of next month, one per CPU.
- Go versions 1.4 and lower, defaults to 1 CPU
- Go versions 1.5 and above, defaults to the number of CPUs visible to the operating system.

From the point of view of the langauge scheduling looks like a function call, and has the same function call semantics. The thread of execution calls into the scheduler with a specific stack, and can return with a different stack.

Compare this to threaded applications, where a thread can be preempted at any time, at any instruction. In Go, the compiler handles this as a natural by product of the function call preamble.

* Goroutines

- Super cheap to create
- Super cheap to switch between as it all happens in user space
- Tens of thousands of goroutines in a single process are the norm, hundreds of thousands not unexpected.

This allows the runtime to spin up a new thread which will service other goroutines while this current thread blocked.

This results in relatively few operating system threads per Go process, with the Go runtime taking care of assigning a runnable Goroutine to a free operating system thread.

* Stack mangement

# In the previous section I discussed how goroutines reduce the overhead of managing many, sometimes hundreds of thousands of concurrent threads of execution.
# There is another side to the goroutine story, and that is stack management.

* Process address space

.image performance-without-the-event-loop/process.png

This is a diagram of the memory layout of a process. The key thing we are interested is the location of the heap and the stack.

# Traditionally inside the address space of a process, the heap is at the bottom of memory, just above the program (text) and grows upwards.

# The stack is located at the top of the virtual address space, and grows downwards.

* Stacks and guard pages

.image performance-without-the-event-loop/guard-page.png

Because the heap and stack overwriting each other would be catastrophic, the operating system usually arranges to place an area of unwritable memory between the stack and the heap to ensure that if they did collide, the program will abort.

# This is called a guard page, and effectively limits the stack size of a process, usually in the order of several megabytes.

* Thread stacks

.image performance-without-the-event-loop/threads.png

Threads share the same address space, so for each thread, it must have its own stack, and its own guard page.

# Because it is hard to predict the stack requirements of a particular thread, a large amount of memory is reserved for each thread’s stack along with a guard page.

# The hope is that this is more than will ever be needed and the guard page will never be hit.

# The downside is that as the number of threads in your program increases, the amount of available address space is reduced.

* Goroutine stack management

The early process model, combined with large 32 bit or 64 bit address spaces, allowed the programmer to view the heap and the stack as effectively infinite. The downside was a complicated and expensive subprocess model.

Threads improved the situation a bit, but require the programmer to _guess_ the most appropriate stack size; too small, your program will abort, too large, you run out of virtual address space.

We’ve seen that the Go runtime schedules a large number of goroutines onto a small number of threads, but what about the stack requirements of those goroutines ?

* Goroutine stack growth

.image performance-without-the-event-loop/stack-growth.png 250 _

Each goroutine starts with an small stack, allocated from the heap. The size has fluctuated over time, but in Go 1.5 each goroutine starts with a 2k allocation.

Instead of using guard pages, the Go compiler inserts a test as part of every function call to check if there is sufficient stack for the function to run.

# If there is sufficient stack space, the function runs as normal.

If there is insuffient space, the runtime will allocate a large stack segment on the heap, copy the contents of the current stack to the new segment, free the old segment, and the function call restarted.

Because of this check, a goroutines initial stack can be made much smaller, which in turn permits Go programmers to treat goroutines as cheap resources.

* Escape analysis

* Allocation

- How many bytes, what is the footprint, per connection, per client, per request ?

Mandatory garbage collection makes Go a simpler and safer language.

This does not imply that garbage collection makes Go slow, or that garbage collection is the ultimate arbiter of the speed of your program.

What it does mean is memory allocated on the heap comes at a cost. It is a debt that costs CPU time every time the GC runs until that memory is freed.

* Heap management through escape analysis

Concurrent, low pause collector, in Go 1.5, but creating garbage will never be free (hint: it's not free with malloc or reference counting either), so you want to avoid that where possible to manage your alloations up front.

There is however another place to allocate memory, and that is the stack.

Unlike C, which forces you to choose if a value will be stored on the heap, via malloc, or on the stack, by declaring it inside the scope of the function, Go implements an optimisation called escape analysis.

Escape analysis determines whether any references to a value escape the function in which the value is declared.

If no references escape, the value may be safely stored on the stack.

Values stored on the stack do not need to be allocated or freed.

Lets look at some examples

Sum adds the numbers between 1 and 100 and returns the result. This is a rather unusual way to do this, but it illustrates how Escape Analysis works.

Because the numbers slice is only referenced inside Sum, the compiler will arrange to store the 100 integers for that slice on the stack, rather than the heap.

There is no need to garbage collect numbers, it is automatically freed when Sum returns.

This second example is also a little contrived. In CenterCursor we create a new Cursor and store a pointer to it in c.

Then we pass c to the Center() function which moves the Cursor to the center of the screen.

Then finally we print the X and Y locations of that Cursor.

Even though c was allocated with the new function, it will not be stored on the heap, because no reference c escapes the CenterCursor function.

Go’s optimisations are always enabled by default. You can see the compiler’s escape analysis and inlining decisions with the -gcflags=-m switch.

Because escape analysis is performed at compile time, not run time, stack allocation will always be faster than heap allocation, no matter how efficient your garbage collector is.

* Integrated network poller

* Servicing network traffic

Reads / writes to socket fd would be a normal blocking operation

- scheduler would let the goroutines' backing thread block, then find / spawn another to continue to service lots of goroutines

Naieve implementation would consume a lot of threads, possibly up to 1 per goroutine, possibly more than one per incoming connection



* Success stories

- Vitess (mysql proxy that sits between every db call behind Youtube)
- dl.google.com
- New York Times
- Hailo
- Facebook/Parse
- Docker
- CoreOS; fleet, etcd, rkt, 
- Kubernetes
- Baidu 
- Qihoo 360
- [[https://github.com/golang/go/wiki/GoUsers][... and many more]]

* Conclusion

* Escape Analysis, Stack management, Goroutines, an Integrated network poller

- Escape Analysis
- Stack management
- Processes and threads vs goroutines
- Integrated network poller

As powerful as these four features are individually, they do not exist in isolation.

For example, the way the runtime multiplexes goroutines onto threads would not be nearly as efficient without growable stacks.

Escape analysis reduces the pressure on the garbage collector by automatically moving allocations from the heap to the stack.

Escape analysis is also provides better cache locality.

Without growable stacks, escape analysis might place too much pressure on the stack.

All of these features are (mostly) transparent to the programmer.

Go lets you write high performance servers with simple, imperative, code.

* But wait, there's more

As this is the performance track, I've mostly constrained my remarks to this area, but there is a lot more to the Go success story than just being fast, and these are arguably the more notable features of the language.

Super fast compilation

Excellent tooling
- go build
- go vet
- go lint
- godoc
- race detector

Excellent cross platform support, cross compilation as well without tears

Single static binary enables a ridiculusly simple deployment story, scp(1) and you're done -- not runtime interpreter like python, ruby, php, java, etc.

# This talk focuses on the last two items, how the language and the runtime transparently let Go programmers write highly scalable network servers without having to worry about thread management or blocking I/O.

# The goal of this talk is to introduce the following features of the language and the runtime:

# These four features work in concert to build an argument for the suitability of Go as a language for writing high performance servers.

# This then is the subject of my talk, features of the Go language and runtime environemnt that (mostly) transparently to the programmer allow you to write high performance servers with simple, structured programming, code.

* Microservices conf ?

I saw a tweet a few days ago commenting that last year OSCON had no microservices talk, this year potentially dozens.

Now, being Australian, I'm naturally sceptical of the hype is building up behind microservices.

However, if you're planning to launch a consumer facing service, market forces are , you _need_ a langauge which is concurrent from the start.

You need a language which is memory efficient, because while concurrecny can be simulated with threads, to hold up an internet's work of traffic you cannot dedicate megabytes of ram per incoming connection, lest your business is dragged under by hardware and overheads.

if you want to write software that supports the entire internet, it is no long possible to do that with a single monolithic application -- there just isn't a machine that you can buy that is large enough


on your application being successful, then it needs to be composable from day one

with that said, any succesful consumer facing web business is going to be decomposing their application, and that decomposition is a market that Go serves very well.

The langauge and the runtime take care of handling blocking i/o, thread scheduling, stack management, memory management.

Simple deployment story
Simple conventions
Excellent network and concurrency support

* Stop, question time
