Writing High Performance Go
GopherChina
17 Apr 2016

Dave Cheney
dave@cheney.net
http://dave.cheney.net/
@davecheney

# Thank organisers

# Thank audience for coming to my talk

* About me

Hello

My name is David. I'm a Go programmer from Sydney, Australia.

I work for Canonical, where I've been writing software in Go for the last four years.

I'm also a contributor to the Go project and I run the Sydney Go Users' group.

* Agenda

This talk is aimed at development teams who are developing production Go applications intended for high scale deployment.

- What does performance mean, what is possible?
- Performance measurement and profiling
- Benchmarking
- Memory management and GC tuning
- Concurrency

* Questions

This presentation will be available after the talk. It contains lots of examples and links to other material.

I have a lot of material to cover in 50 minutes, so I ask that you hold your questions til the end.

I'm going to be around for the entire conference, please come and ask me your questions after the talk.

* What does performance mean, what is possible?

* What does performance mean?

What do we mean when we say performance ?

How do we know when we are fast ?

Before we start to talk about writing high performance code, we must establish an understanding of the current trends in computing hardware and outline the issues facing server software development.

From these factors we can draw a map of the areas where application performance can be won and lost.

* The hardware

Before we talk about writing high performance code, we need to talk about the hardware that will execute this code.

What are it's properties, how has to changed over time ?

As software authors we have benefitted from Moore's Law, the doubling of the number of availble transitors on a chip every 18 months for 50 years now.

No other industry has experienced 4, sometimes 5 order of magnitude improvement in their tools in the space of less than a lifetime.

But that is all changing

* The CPU

.html writing-high-performance-go/cpu.svg

Clock speeds have not increased in a decade.

* More cores

.image writing-high-performance-go/Nehalem_Die_Shot_3.jpg _ 600

Over the last decade performance, especially on the server is dictated by multi core performance.

* Memory

Physical memory attached to a server has increased geometrically.

.image writing-high-performance-go/latency.png _ 600

But, in terms of processor cycles lost, physical memory is still as far away as ever.

# memory is physically distant from the CPU.

* Cache rules everything around it

Cache rules everything around it, but it is small, and will remain small because the speed of light determines how large a cache can been at a certain latency.

You can have a larger cache, but it will be slower, because in a universe where electricity travels a foot every nano second, distance equals latency.

We already have three caches:

- *L1*, per CPU core, 64 kilobytes
- *L2*, per set of cores, usually 2-4 cores, 256 kilobytes - 1 megabyte
- *L3*, per CPU package, 4 - 32 megabytes

* Network I/O and disk I/O are still expensive

Network I/O and disk I/O are still expensive, so expensive that the Go runtime will schedule something else while those operations are in progress.

.image writing-high-performance-go/numbers.png _ 600

* The free lunch is over

In 2005, Herb Sutter, the C++ committee leader wrote an article entitled [[http://www.gotw.ca/publications/concurrency-ddj.htm][_The_free_lunch_is_over_]].

In the articled Sutter discussed all the points I have just made and drew the attention of programmers to the fact that they could not simply rely on faster hardware to solve problems of slow programs—or slow programming languages.

Now, a decade later, there is no doubt that Herb Sutter was right. Memory is slow, caches are too small, CPU clock speeds are going backwards, and the simple world of a single threaded CPU is long gone.

* A fast programming langauge

So, it's time, for the software to come to the party.

As Rick Hudson spoke in at GopherCon in 2015, it's time for a programming language that works _with_ the limitations of today's hardware, rather than continue to ignore the reality that CPU designers find themselves.

So, for best performance on todays hardware in todays world, you need a programming language which:

- Is compiled, not interpreted.
- Has a good compiler that produces efficient code, it has to be small code as well, because cache.
- Has to permit efficient code to be written.
- Has to let programmers talk about memory effectively, think structs vs java objects

Obviously, the language I'm talking about is the one we're here to discuss, Go.

* Performance measurement and profiling
 
* Performance measurement and profiling

There is a common saying in Australia, I'm sure you have your own version of it.

_"Measure_twice,_cut_once"_

Before we can begin to tune your application, we must first know how to tell if what we are doing is making things better, or worse. We must first establish a reliable baseline to measure the impact of your change.

In other words, _"Don't_guess,_measure"_

* Profiling basics

Before you profile, you have to have a stable environment to get repeatable results

- The machine must be idle—don't profile on shared hardware.
- Don't browse the web while waiting for a long benchmark to run.
- Watch out for power saving and thermal scaling.
- Avoid virtual machines and shared cloud hosting; they are too noisy for consistent measurements.
- There is a kernel bug on OS X versions less than El Capitan; upgrade or avoid profiling on OS X.

If you can afford it, buy dedicate performance test hardware. Rack them, disable all the power management and thermal scaling and never update the software on those machines.

For everyone else, have a before and after sample and run them both multiple times to get consistent results.

* pprof

The primary tool we're going to be talking about today is _pprof_.

pprof decends from the Google Perf Tools suite of tools.

pprof profiling is built into the Go runtime.

pprof supports three types of profiling:

- cpu
- memory
- block

* CPU profiling

CPU profiling is the most common type of profile, and the most obvious. 

When enabled the runtime will interrupt itself every 10ms and record the stack trace of the goroutines were running at the time.

The more times a fuction appears in the profile, the more time it takes.

* Memory profilng

Memory profiling records the stack trace when a _heap_ allocation is made.

Stack allocate is assumed to be free and is _not_tracked_ in the memory profile. We'll talk more about this when we discuss escape analysis.

Memory profiling, like CPU profiling is sample based, by default memory profiling samples 1 in every 1000 allocations. This rate can be changed.

Because of sampling, and because this tracks _allocations_ not _use_, using a memory profile to determine overall memory usage is very difficult.

_Note:_Personal_Opinion:_ Personally I do not find memory profiling useful for finding memory leaks. There are better ways to determine how much memory your application is using. We will discuss these later in the presentation.

* Block profiling

Block profiling is quite unique. 

A block profile is similar to a CPU profile, but it records the amount of time a goroutine spent waiting for a shared resource.

* Using pprof

pprof should always be invoked with _two_ arguments

    go tool pprof /path/to/your/binary /path/to/your/profile

The `binary` argument *must* be the binary that produced this profile.

The `profile` argument *must* be the profile generated by this binary.

*Warning*: Because pprof also supports an online mode where it can fetch profiles from a running application over http, the pprof tool can be invoked without the name of your binary:

    go tool pprof /tmp/c.pprof

*Do*not*do*this*or**pprof*will*report*your*profile*is*empty.*

* Using pprof (cont.)

	% go tool pprof application /tmp/c.p
	Entering interactive mode (type "help" for commands)
	(pprof) top
	Showing top 15 nodes out of 63 (cum >= 4.85s)
	      flat  flat%   sum%        cum   cum%
	    21.89s  9.84%  9.84%    128.32s 57.71%  net.(*netFD).Read
	    17.58s  7.91% 17.75%     40.28s 18.11%  runtime.exitsyscall
	    15.79s  7.10% 24.85%     15.79s  7.10%  runtime.newdefer
	    12.96s  5.83% 30.68%    151.41s 68.09%  test_frame/connection.(*ServerConn).readBytes
	    11.27s  5.07% 35.75%     23.35s 10.50%  runtime.reentersyscall
	    10.45s  4.70% 40.45%     82.77s 37.22%  syscall.Syscall
	     9.38s  4.22% 44.67%      9.38s  4.22%  runtime.deferproc_m
	     9.17s  4.12% 48.79%     12.73s  5.72%  exitsyscallfast
	     8.03s  3.61% 52.40%     11.86s  5.33%  runtime.casgstatus
	     7.66s  3.44% 55.85%      7.66s  3.44%  runtime.cas
	     7.59s  3.41% 59.26%      7.59s  3.41%  runtime.onM
	     6.42s  2.89% 62.15%    134.74s 60.60%  net.(*conn).Read
	     6.31s  2.84% 64.98%      6.31s  2.84%  runtime.writebarrierptr
	     6.26s  2.82% 67.80%     32.09s 14.43%  runtime.entersyscall
	     4.85s  2.18% 69.98%      4.85s  2.18%  save

Often this output is hard to understand.

* Using pprof (cont.)

pprof also supports these modes in a non interactive form with flags like `-svg`, `-pdf`, etc. See `go tool pprof --help` for more details.

	% go tool pprof application /tmp/c.p
	Entering interactive mode (type "help" for commands)
	(pprof) web

Opens a web page with a graphical display of the profile.

.link writing-high-performance-go/profile.svg

* One profile at at time

Profiling is not free, it has a moderate, but messurable impact on program performance—especially if you increase the memory profile sample rate.

Most tools will not stop you from enabling multiple profiles at once, do not do this.

If you enable multiple profile's at the same time, they will observe their own interactions and throw off your results.

*Do*not*enable*more*than*one*kind*of*profile*at*a*time.*

* How to profile

Profiling is controlled by the `runtime/pprof` package.

`runtime/pprof` is a very low level tool, and for historic reasons the interfaces to each kind of profile are not uniform.

You should use a higher level 

For profiling `testing` package benchmarks, the `go`test` command has integrated support for profiling the code under test:

    go test -run=XXX -bench=. -cpuprofile=/tmp/c.p

For profiling an application, I recommend the [[https://github.com/pkg/profile][github.com/pkg/profile]] package.

We'll look at both of these in more detail in the next section.

* Benchmarking

* Benchmarking

Now that we discussed what profiling is, and how to use `pprof`, we're going to look at writing benchmarks and interpreting their results.

This section focuses on how to construct useful benchmarks using the Go testing framework, and gives practical tips for avoiding the many pitfalls of this task.

* Using the testing package for benchmarking

fib.go:
.code writing-high-performance-go/fib/fib_test.go /STARTFIB OMIT/,/ENDFIB OMIT/

fib_test.go:
.code writing-high-performance-go/fib/fib_test.go /STARTBENCH OMIT/,/ENDBENCH OMIT/

.link http://dave.cheney.net/2013/06/30/how-to-write-benchmarks-in-go

* Beware the compiler

How fast will this benchmark run ?

.code writing-high-performance-go/popcnt/popcnt_test.go /START OMIT/,/END OMIT/

* What happened

`popcnt` is a leaf function, so the compiler can inline it.

Because the function is inlined, the compiler can see it has no side effects; it is a _pure_function_.

This is only going to get stronger. Because the same optimisations that make real code fast, by removing unnecessary computation are the same ones that remove benchmarks that have no observable side effects.

* Profiling applications

Show including profile in cmd/godoc

* Comparing benchmarks

benchcmp, show before and after, show variation due to thermal scaling
benchstat, with -count to eliminate the variations
benchvis if you want to produce nice graphs for your boss. TODO(dfc) does this work with benchstat output

* Inlining

Show how inlining works with examples

* Memory management and GC tuning

* Memory management and GC tuning

As a garbage collected language, the performance of Go programs is often determined by their interaction with the garbage collector.

Next to your choice of algorythms, memory consumption is the most important factor that determines the performance and scalability of your application.

This section discusses the operation of the garbage collector, how to measure the memory usage of your program and strategies for lowering memory usage if garbage collector performance is a bottleneck.

* Garbage collector design

Go is a garbage collected language. This is a design principal, it will not change.

The design of the Go GC has changed over the years

- Go 1.0, stop the world mark sweep collector based heavily on tcmalloc
- Go 1.2, precise collector, wouldn't mistake big numbers (or big strings of text) for pointers
- Go 1.3, now fully precise tracking of all stack values.
- Go 1.4, mark and sweep now parallel, but still stop the world.
- Go 1.5, new gc design, focusing on _latency_ over _throughput_.
- GO 1.6, gc improvements, handling larger heaps with lower latency.

* Garbage collector design (cont.)

Stop the world mark sweap is the most efficient GC in terms of total run time; scientific job, simulation, etc.

The Go GC is designed to favor lower latency over maximum throughput; it moves some of the cost of allocation to the mutator to reduce the cost of cleanup later.

This is designed for low latency applications, servers and (hopefully) interactive applications.

* Garbage collector measurement

Show

GODEBUG=gogctrace=1

Show gcvis

Show /debug/pprof/heap

* Garbage collector tuning

The Go runtime provides one variable to tune the GC, `GOGC`

    goal = reachable * (1 + GOGC/100)


* Reduce allocations

Show how to pass data in to a function
Show how to reuse arrays as buffers

Show a simple loop that does a make([]byte, 4096) and then decodes

* strings and []bytes

strings are immutable, []bytes are not.

This means most IO is done with []byte, and but most programs prefer to work with strings

- Avoid slice to string conversions wherever possible, this normally means picking one represetation, either a string or a []byte for a value. Often this will be []byte
- The bytes package contains many of the same oerations, subtring, split, prefix, trim, as the strings package. Under the hood strings uses same assembly primatives as the bytes package anway

* Using []byte as a map key

It is very common to use a string as a map key, but often you have a []byte

The compiler implements a specific optimisation for this case

v, ok := m[string(bytes)]

This will avoid the conversion of the byte slice to a string for the map lookup. This is very specific, it won't work if you do something like

key := string(bytes)
val, ok := m[key] 

* Use bytes.Buffer to build strings



* Using sync.Pool

How to use sync.Pool

Warn that it is not a cache

Show that it is emptied

* Escape analysis

Show how escape analysis works

Escape analysis cannot see through interfaces

* Concurrency

Go's signature feature is its lightweight concurrency model.

While cheap, these features are not free, and their overuse often leads to unexpected performance problems.

This final section concludes with a set of do's and don't's for efficient use of Go's concurrency primitives, including management of native operating system resources and avoiding memory leaks associated with lost goroutines. 

* Goroutines

The key feature of Go that makes it great for server applications is goroutines.

No java.util.ThreadPoolExecutor, no node.js function() callbacks, just write simple goroutines and let the Go runtime take care of multiplexing them onto the hardware.

Gorountines are so easy to use, and so cheap to create, you could think of them as almost _free_. 

Indeed the Go runtime has been written for tens of thousands of goroutines as the norm, hundreds of thousands are not unexpected.

However, each goroutine does consume a minimum amount of memory in runtime tables and in the goroutines' stack which is at least 2k.

2048 * 1,000,000 goroutines == 2Gb of memory per 1,000,000 goroutines.

# Maybe this is a lot, maybe it isn't given the other usages of your application

* Know when to stop a goroutine

Goroutines are cheap to start and cheap to run, but they do have a finite cost in terms of memory footprint; you cannot create an infinite number of them. So I present to you a rule

> Never start a goroutine without knowing how it will stop.

Channel ownership, who owns the channel, only the owner may close it. 

You do not need to close a channel for it to be garbage collected, that will happen once every reference to your channel has been discarded. 

Closing the channel is a signal, soem metadata apart from the channel values themselves

But the two are related, because most consumers wait for channel to be closed, and won't exit, thus drop their reference to this channel.until it is closed. 

From this we draw three recommendations

Only the owner of a chanel may close the chabel / A channel may only be closed by its owner

> and a channel must have at most one owner

Never start a goroutine until you know how it will stop. 

* Input / Output

> The UNIX kernel is an I/O multiplexer more than a
complete operating system. This is as it should be.”

Ken Thompson. Unix implementation. Bell System Technical Journal, 57(6, Part 2):1931–1946, July/August 1978.

* IO

As you're writing a server process, its primary job is to be an intermediary between clients connecting over the network, and data stored in your application.

If memory is slow, relatively speaking, then IO is so slow that you should avoid doing it at all costs. 

Most importantly avoid doing IO in the context of a request -- don't make the user wait for your disk subsystem to write to disk, or even read. That is a sure recipe for overload.

* IO numbers

Here are the numbers to pay attention to

- The amount of IO requests per incoming request; how many IO events does a single client request generate. It might be on average 1, or possibly less than one if many requests are served out of a cache

- The amount of reads required to service a query; is it fixed, linear (N+1), or exponential (reading the whole table to generate the last page of results).

* Use streaming IO interfaces

Whereever possible use io.Reader and io.Writer; avoid reading into a []byte and passing that around

For efficiency, consider implementing io.ReaderFrom / WriterTo if you use a lot of io.Copy, these are more efficient and avoid copying memory into a temporary buffer.

* Copying

Is copying bad ?

If your servers' job is to read from one place and write to another, then you want to limit the amount of handling of that data as possile to reduce. So yes, in this case unnecessary copying is bad.

BUT, I do not believe you can say all copying is bad, for example, this piece of code

    type T struct {
           A int
           B int
           C int
    }

    func fn(t T) // takes a copy of t, not a pointer

Is this bad ? Would you rewrite this code to pass the T in as a pointer ?

[ take show of hands ]

What about this code

    func fn(t []string) 

A slice value is the same size as T; three machine words, and we don't think anything of passing slices into functions.

Clearly there is a limit below which copying is not a problem, and intel CPU's are _very_ good at doing bulk memory copy, so please don't go passing around every parameter as a pointer because you think it will avoid copying.

* Go uses efficient network polling for some requests

For network IO, things that implement the net.Conn interfaces that you get from the net package are handled efficiently by the runtime using kqueue, epoll, or windows IOCP.

This is all transpant to you as the user.

However, for disk IO, Go does not implement any io polling -- each os.File operation will consume one OS thread during operation.

Your disk subsystem does not expect to be able to handle hundreds or thousands of concurrent IO requests. It will be very slow and that will cause your go program to spawn hunderds or thousands of threads to service those blocked IO routines.

* io.Reader and io.Writer are not buffered

`io.Reader` and `io.Writer` implementations are not buffered.

This includes `net.Conn` and `os.Stdout`.

Use `bufio.NewReader(r)` and `bufio.NewWriter(w)` to get a buffered reader and writer.

Don't forget to `Flush` or `Close` your buffered writers to flush the buffer to the underlying `Writer`.

* Timeouts, timeouts, timeouts

Never start an IO operating without knowing the maximum time it will take.

You need to set a timeout on every network request you make

You need to limit the amount of blocking IO requests in progress as not portable 

* Minimise CGO

.link http://dave.cheney.net/2016/01/18/cgo-is-not-go

* TODO

- Singleflight

* Conclusion

* Always write the simplest code you can

Start with the simplest possible code.

Measure.

If performance is good, then _stop_, you don't need to optimise everything, only the hottest parts of your code.

As you application grows, or your traffic pattern evolves, the performance hot spots will change.

- Don't leave complex code that is not perfromance critical, rewrite it with simpler operationgs

* The compiler is optimised for normal code

All the optimisations going into the Go compiler are targeting _normal_ code.

.code writing-high-performance-go/copy/copy_test.go /START OMIT/,/END OMIT/

Which is faster ? Turns out they are the same in performance. 

Which is more understandable ?

* Don't trade performance for reliability

Most of these tips are about performance, but some of them are about reliability, becsuse I see little value in having a very fast server that panics, deadlocks or blows it's memory cap on a regular basis.

Performance and reliability are equally important. 

* Always use the latest released version of Go

Old versions of Go will not get faster.

Go 1.4 had a faster compiler, but the garbage collector does not scale as well as Go 1.6.

Go 1.5 and 1.6 did have a slower compiler, but it produced faster code.

We will fix the speed of the compiler, and the code produced by the compiler will always be better

Old version of Go receive no updates, do not stay on them, use the latest and you will get the fastest programs.

* Go 1.7 compiler performance

.image writing-high-performance-go/go17burndown.png _ 950

I'm sorry about Go 1.5 and Go 1.6 compile speed, believe me, nobody is happy with the situation and we are working on improving it.

* China is a big market, and a big market for go.

You in this room are going to be pushing Go hard.

* In general

Shorter code is faster code.

Shorter code is _smaller_ code; which is important from L1-D$.

Avoid assignment, avoid jumps, avoid loops, avoid branches.

Eliminating a statement with a smarter algorithm will pay off

Show owner, newer example

* When to optimise

> “We should forget about small efficiencies, say about 97% of the time; premature optimization is the root of all evil” – Donald E. Knuth,

https://www.cs.sjsu.edu/~mak/CS185C/KnuthStructuredProgrammingGoTo.pdf

.image knuth

Don't do it.

Don't do it, _yet_.

Always, start by writing the simplest, easiest to understand; easiest to read code, then optimise.

# now, I know that you run big servers here in china, that’s why you’re interested in go -- it’s a language for servers. So for the rest of this talk, don’t worry about the man in the chair, we’re going to talk about how to get the most out of your servers.

 ---

show benchmarks
show net/http/pprof
talk about -benchmem, allocs/op and bytes/op, allocs per op is the cost to allocate the memory, bytes/op is the cost to clean it up -- sort of, you want both to be low.
benchmarking means running something small hundreds of thousands of times

---

memory, 
where does it come from; storage classes - global, stack heap
in order of flexiblity and order of cost
memory has a cost to allocate and a cost to free, if you want to avoid paying the cost, don’t free it
writing code the compiler can compile
pointers are expensive
prefer slices to maps or linked lists
a little copying is less bad than always pointer chasing. copying has a fixed cost relative to the size of the thing you copy, pointer chasing gets more expensive the larger your heap because the cache becomes less effective.
talk about goroutine management
dont’ start a goroutine unless you know how to stop it
maybe that is when the program exits, but usualy if this goroutine is for a client, you need to have some rasonable ways to killing it after a certain time
http.esrver, use timeouts

---

* Memory numbers

If you are writing a server measure and know the memory footprint

- Per record stored or retrieve
- Per incoming request
- Per concurrent connection

